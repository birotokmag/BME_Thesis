{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV as gscv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-6e81f529eb55>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ratios[y+i+\"mf\"] = df[y+i]/df[y+\"TAH061\"]\n",
      "<ipython-input-2-6e81f529eb55>:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ratios[y+i+\"liab\"] = df[y+i]/(df[y+\"TAH051\"]+df[y+\"TAH054\"])\n",
      "<ipython-input-2-6e81f529eb55>:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ratios[y+i+\"toke\"] = df[y+i]/(df[y+\"TAH001\"]+df[y+\"TAH012\"]+df[y+\"TAH048\"]+df[y+\"TAH189\"]+df[y+\"TAH208\"]+df[y+\"TAH060\"]+df[y+\"TAH179\"]+df[y+\"TAH187\"]+df[y+\"TAH059\"])\n",
      "<ipython-input-2-6e81f529eb55>:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ratios[y+i+\"kolt\"] = df[y+i]/(df[y+\"TAC002\"]+df[y+\"TAC006\"]-df[y+\"TAC019\"])\n",
      "<ipython-input-2-6e81f529eb55>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ratios[y+i+\"arbev\"] = df[y+i]/df[y+\"TAC002\"]\n",
      "<ipython-input-2-6e81f529eb55>:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cRatios[str(y)+\"c\"+str(y+1)+i+\"mf\"] = (ratios[str(y+1)+i+\"mf\"]-ratios[str(y)+i+\"mf\"])/ratios[str(y)+i+\"mf\"]\n",
      "<ipython-input-2-6e81f529eb55>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cRatios[str(y)+\"c\"+str(y+1)+i+\"liab\"] = (ratios[str(y+1)+i+\"liab\"]-ratios[str(y)+i+\"liab\"])/ratios[str(y)+i+\"liab\"]\n",
      "<ipython-input-2-6e81f529eb55>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cRatios[str(y)+\"c\"+str(y+1)+i+\"toke\"] = (ratios[str(y+1)+i+\"toke\"]-ratios[str(y)+i+\"toke\"])/ratios[str(y)+i+\"toke\"]\n",
      "<ipython-input-2-6e81f529eb55>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cRatios[str(y)+\"c\"+str(y+1)+i+\"kolt\"] = (ratios[str(y+1)+i+\"kolt\"]-ratios[str(y)+i+\"kolt\"])/ratios[str(y)+i+\"kolt\"]\n",
      "<ipython-input-2-6e81f529eb55>:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cRatios[str(y)+\"c\"+str(y+1)+i+\"arbev\"] = (ratios[str(y+1)+i+\"arbev\"]-ratios[str(y)+i+\"arbev\"])/ratios[str(y)+i+\"arbev\"]\n"
     ]
    }
   ],
   "source": [
    "nCluster=8\n",
    "df = pd.read_pickle(\"data/dataframe\")\n",
    "yearsString = [\"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"]\n",
    "arbevetel = [\"TAH197\"]\n",
    "koltseg = [\"TAC008\",\n",
    "\"TAC009\",\"TAC011\",\"TAC010\",\"TAC078\",\"TAH014\",\"TAH015\",\"TAC012\",\"TAC013\",\"TAC014\",\"TAC015\",\"TAC016\",\"TAC018\",\"TAC063\",\"TAH021\",\"TAC109\",\"TAC190\",\"TAC059\",\"TAB024\",\"TAH197\"]\n",
    "merlegfoosszeg=[\"TAH186\",\n",
    "\"TAH230\",\"TAH033\",\"TAH087\",\"TAH088\",\"TAH041\",\"TAH042\",\"TAH231\",\"TAH043\",\"TAH044\",\"TAH232\",\"TAH045\", \"TAH187\", \"TAI001\",\n",
    "\"TAI002\",\"TAI019\",\"TAI020\",\"TAI022\",\"TAI023\",\"TAI041\",\"TAI042\",\"TAI043\",\"TAI044\",\"TAI033\",\"TAH034\",\"TAI045\",\"TAI046\",\"TAI035\",\"TAI047\",\"TAI048\",\"TAI036\",\"TAI049\",\"TAI050\",\"TAI037\",\"TAI051\",\"TAI052\",\"TAI053\",\"TAI054\",\"TAI055\",\"TAI056\",\"TAI057\",\"TAH190\",\"TAI038\",\"TAI058\", \"TAB024\",\"TAH197\"]\n",
    "liabilities = [\"TAH209\",\"TAH051\",\"TAH227\",\"TAH054\",\"TAH055\",\"TAH180\",\"TAH199\"]\n",
    "toke = [\"TAH002\",\"TAH003\",\"TAH004\",\"TAH005\",\"TAH011\",\"TAH007\",\"TAH233\",\"TAH234\",\"TAH012\",\"TAH048\",\"TAH189\",\"TAH208\",\"TAH060\",\"TAH179\",\"TAH187\"]\n",
    "\n",
    "ratios = pd.DataFrame()\n",
    "for y in yearsString:\n",
    "    for i in arbevetel:\n",
    "        ratios[y+i+\"arbev\"] = df[y+i]/df[y+\"TAC002\"]\n",
    "    for i in merlegfoosszeg:\n",
    "        ratios[y+i+\"mf\"] = df[y+i]/df[y+\"TAH061\"]\n",
    "    for i in liabilities:\n",
    "        ratios[y+i+\"liab\"] = df[y+i]/(df[y+\"TAH051\"]+df[y+\"TAH054\"])\n",
    "    for i in toke:\n",
    "        ratios[y+i+\"toke\"] = df[y+i]/(df[y+\"TAH001\"]+df[y+\"TAH012\"]+df[y+\"TAH048\"]+df[y+\"TAH189\"]+df[y+\"TAH208\"]+df[y+\"TAH060\"]+df[y+\"TAH179\"]+df[y+\"TAH187\"]+df[y+\"TAH059\"])\n",
    "    for i in koltseg:\n",
    "        ratios[y+i+\"kolt\"] = df[y+i]/(df[y+\"TAC002\"]+df[y+\"TAC006\"]-df[y+\"TAC019\"])\n",
    "ratios = ratios.replace(np.inf, 0)\n",
    "ratios = ratios.replace(-np.inf, 0)        \n",
    "ratios.fillna(0, inplace=True)\n",
    "        \n",
    "cRatios = pd.DataFrame()\n",
    "for y in range(14, 20):\n",
    "    for i in arbevetel:\n",
    "        cRatios[str(y)+\"c\"+str(y+1)+i+\"arbev\"] = (ratios[str(y+1)+i+\"arbev\"]-ratios[str(y)+i+\"arbev\"])/ratios[str(y)+i+\"arbev\"]\n",
    "    for i in merlegfoosszeg:\n",
    "        cRatios[str(y)+\"c\"+str(y+1)+i+\"mf\"] = (ratios[str(y+1)+i+\"mf\"]-ratios[str(y)+i+\"mf\"])/ratios[str(y)+i+\"mf\"]\n",
    "    for i in liabilities:\n",
    "        cRatios[str(y)+\"c\"+str(y+1)+i+\"liab\"] = (ratios[str(y+1)+i+\"liab\"]-ratios[str(y)+i+\"liab\"])/ratios[str(y)+i+\"liab\"]\n",
    "    for i in toke:\n",
    "        cRatios[str(y)+\"c\"+str(y+1)+i+\"toke\"] = (ratios[str(y+1)+i+\"toke\"]-ratios[str(y)+i+\"toke\"])/ratios[str(y)+i+\"toke\"]\n",
    "    for i in koltseg:\n",
    "        cRatios[str(y)+\"c\"+str(y+1)+i+\"kolt\"] = (ratios[str(y+1)+i+\"kolt\"]-ratios[str(y)+i+\"kolt\"])/ratios[str(y)+i+\"kolt\"]\n",
    "\n",
    "cRatios = cRatios.replace(np.inf, 0)\n",
    "cRatios = cRatios.replace(-np.inf, 0)        \n",
    "cRatios.fillna(0, inplace=True)\n",
    "\n",
    "prodInTime = pd.DataFrame()\n",
    "for i in range(14, 21): \n",
    "    prodInTime[str(i)] = df[str(i)+\"Productivity\"]\n",
    "prodInTime = TimeSeriesScalerMeanVariance().fit_transform(prodInTime)\n",
    "\n",
    "timeSeriesClusterModel = TimeSeriesKMeans(n_clusters=nCluster, metric=\"dtw\", max_iter=10, random_state=15)\n",
    "timeSeriesClusterModel.fit(prodInTime)\n",
    "targetLabels = timeSeriesClusterModel.predict(prodInTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainX = []\n",
    "ctrainY = []\n",
    "cstackX = []\n",
    "cstackY = []\n",
    "ctestX = []\n",
    "ctestY = []\n",
    "for c in range(0, nCluster):\n",
    "    cRatiosMask = np.load(\"cRatiosRFEMaskC\"+str(c)+\".npy\")\n",
    "    cRatiosCluster = cRatios[cRatios.columns[cRatiosMask]]\n",
    "    train_X, rem_X, train_Y, rem_Y = train_test_split(cRatiosCluster, targetLabels == c, test_size=0.5, random_state=42)\n",
    "    stack_X, test_X, stack_Y, test_Y = train_test_split(rem_X, rem_Y, test_size=0.3, random_state=42)\n",
    "    ctrainX.append(train_X)\n",
    "    ctrainY.append(train_Y)\n",
    "    cstackX.append(stack_X)\n",
    "    cstackY.append(stack_Y)\n",
    "    ctestX.append(test_X)\n",
    "    ctestY.append(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrainX = []\n",
    "ntrainY = []\n",
    "nstackX = []\n",
    "nstackY = []\n",
    "ntestX = []\n",
    "ntestY = []\n",
    "for c in range(0, nCluster):\n",
    "    ratiosMask = np.load(\"ratiosRFEMaskC\"+str(c)+\".npy\")\n",
    "    ratiosCluster = ratios[ratios.columns[ratiosMask]]\n",
    "    train_X, rem_X, train_Y, rem_Y = train_test_split(ratiosCluster, targetLabels == c, test_size=0.5, random_state=42)\n",
    "    stack_X, test_X, stack_Y, test_Y = train_test_split(rem_X, rem_Y, test_size=0.3, random_state=42)\n",
    "    ntrainX.append(train_X)\n",
    "    ntrainY.append(train_Y)\n",
    "    nstackX.append(stack_X)\n",
    "    nstackY.append(stack_Y)\n",
    "    ntestX.append(test_X)\n",
    "    ntestY.append(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 482  223]\n",
      " [ 374 1002]]\n",
      "0.7704728950403691\n"
     ]
    }
   ],
   "source": [
    "ratiosMask = np.load(\"ratiosRFEMask.npy\")\n",
    "ratiosCluster = ratios[ratios.columns[ratiosMask]]\n",
    "tL = np.logical_or(np.logical_or(targetLabels == 4, targetLabels == 7), np.logical_or(targetLabels == 3, targetLabels == 1))\n",
    "train_X, rem_X, train_Y, rem_Y = train_test_split(ratiosCluster, tL, test_size=0.5, random_state=42)\n",
    "stack_X, test_X, stack_Y, test_Y = train_test_split(rem_X, rem_Y, test_size=0.3, random_state=42)\n",
    "sm = SMOTE(random_state=42)\n",
    "x_res, y_res = sm.fit_resample(train_X, train_Y)\n",
    "gbc = GradientBoostingClassifier(max_depth = 12, n_estimators = 45)\n",
    "gbc.fit(x_res, y_res)\n",
    "rfc = RandomForestClassifier(max_depth = 14, n_estimators = 40)\n",
    "rfc.fit(x_res, y_res)\n",
    "lrm = LogisticRegression(solver='saga', C=4.5, max_iter=5000)\n",
    "lrm.fit(x_res, y_res)\n",
    "sm = SMOTE(random_state=42)\n",
    "x_res, y_res = sm.fit_resample(stack_X, stack_Y)\n",
    "estimators = [\n",
    "        ('gbc', gbc),\n",
    "        ('rf', rfc),\n",
    "        ('lr', lrm) ]\n",
    "stackingModel = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=10000), cv='prefit', passthrough = True)\n",
    "stackingModel.fit(x_res, y_res)\n",
    "print(metrics.confusion_matrix(test_Y, stackingModel.predict(test_X)))\n",
    "print(metrics.f1_score(test_Y, stackingModel.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 409  296]\n",
      " [ 278 1098]]\n",
      "0.7927797833935019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ratiosMask = np.load(\"ratiosRFEMask.npy\")\n",
    "ratiosCluster = ratios[ratios.columns[ratiosMask]]\n",
    "tL = np.logical_or(np.logical_or(targetLabels == 4, targetLabels == 7), np.logical_or(targetLabels == 3, targetLabels == 1))\n",
    "train_X, rem_X, train_Y, rem_Y = train_test_split(ratiosCluster, tL, test_size=0.5, random_state=42)\n",
    "stack_X, test_X, stack_Y, test_Y = train_test_split(rem_X, rem_Y, test_size=0.3, random_state=42)\n",
    "sm = SMOTE(random_state=42)\n",
    "x_res, y_res = sm.fit_resample(train_X, train_Y)\n",
    "gbc = GradientBoostingClassifier(max_depth = 12, n_estimators = 45)\n",
    "gbc.fit(x_res, y_res)\n",
    "rfc = RandomForestClassifier(max_depth = 14, n_estimators = 40)\n",
    "rfc.fit(x_res, y_res)\n",
    "lrm = LogisticRegression(solver='saga', C=4, max_iter=5000)\n",
    "lrm.fit(x_res, y_res)\n",
    "sm = SMOTE(random_state=42)\n",
    "x_res, y_res = sm.fit_resample(stack_X, stack_Y)\n",
    "estimators = [\n",
    "        ('gbc', gbc),\n",
    "        ('rf', rfc),\n",
    "        ('lr', lrm) ]\n",
    "stackingModel = VotingClassifier(estimators=estimators, voting='soft')\n",
    "stackingModel.fit(x_res, y_res)\n",
    "print(metrics.confusion_matrix(test_Y, stackingModel.predict(test_X)))\n",
    "print(metrics.f1_score(test_Y, stackingModel.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x1148d6dfee0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.RocCurveDisplay.from_predictions(test_Y, stackingModel.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting with SMOTE, change values, RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientResultSmote=joblib.load(\"gradientResultSMOTE.pkl\")\n",
    "gradientModels = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ctrainX[c], ctrainY[c])\n",
    "    parameters = gradientResultSmote[c].best_params_\n",
    "    gbc = GradientBoostingClassifier(**parameters)\n",
    "    gbc.fit(x_res, y_res)\n",
    "    gradientModels.append(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with SMOTE, change values, RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestResultSmote=joblib.load(\"randomForestResultSMOTE.pkl\")\n",
    "rfModels = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ctrainX[c], ctrainY[c])\n",
    "    parameters = randomForestResultSmote[c].best_params_\n",
    "    rfc = RandomForestClassifier(**parameters)\n",
    "    rfc.fit(x_res, y_res)\n",
    "    rfModels.append(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with SMOTE, change values, RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lrResultSmote=joblib.load(\"lrResultSMOTE.pkl\")\n",
    "lrModels = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ctrainX[c], ctrainY[c])\n",
    "    parameters = lrResultSmote[c].best_params_\n",
    "    lrm = LogisticRegression(solver='saga', **parameters, max_iter=5000)\n",
    "    lrm.fit(x_res, y_res)\n",
    "    lrModels.append(lrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "stackingClass = []\n",
    "stackingVote = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(cstackX[c], cstackY[c])\n",
    "    estimators = [\n",
    "        ('gbc', gradientModels[c]),\n",
    "        ('rf', rfModels[c]),\n",
    "        ('lr', lrModels[c]) ]\n",
    "    stackingModel = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=5000), cv='prefit', passthrough = True)\n",
    "    stackingClass.append(stackingModel.fit(x_res, y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1476  437]\n",
      " [  83   85]]\n",
      "0.24637681159420288\n",
      "--------------\n",
      "1\n",
      "[[1376  487]\n",
      " [ 153   65]]\n",
      "0.16883116883116883\n",
      "--------------\n",
      "2\n",
      "[[1411  487]\n",
      " [ 101   82]]\n",
      "0.2180851063829787\n",
      "--------------\n",
      "3\n",
      "[[1352  456]\n",
      " [ 190   83]]\n",
      "0.2044334975369458\n",
      "--------------\n",
      "4\n",
      "[[1240  466]\n",
      " [ 196  179]]\n",
      "0.3509803921568628\n",
      "--------------\n",
      "5\n",
      "[[1461  456]\n",
      " [  85   79]]\n",
      "0.22603719599427752\n",
      "--------------\n",
      "6\n",
      "[[1515  376]\n",
      " [ 118   72]]\n",
      "0.225705329153605\n",
      "--------------\n",
      "7\n",
      "[[1149  422]\n",
      " [ 237  273]]\n",
      "0.4531120331950207\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for c in range(0, nCluster):\n",
    "    print(c)\n",
    "    print(metrics.confusion_matrix(ctestY[c], stackingClass[c].predict(ctestX[c])))\n",
    "    print(metrics.f1_score(ctestY[c], stackingClass[c].predict(ctestX[c])))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stackingVote = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ctrainX[c], ctrainY[c])\n",
    "    voteModel = VotingClassifier(estimators = [\n",
    "        ('gbc', gradientModels[c]),\n",
    "        ('rf', rfModels[c]),\n",
    "        ('lr', lrModels[c]),\n",
    "        ('nb', GaussianNB())], voting='hard')\n",
    "    stackingVote.append(voteModel.fit(x_res, y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1576  337]\n",
      " [  95   73]]\n",
      "0.2525951557093426\n",
      "--------------\n",
      "1\n",
      "[[1539  324]\n",
      " [ 187   31]]\n",
      "0.10820244328097732\n",
      "--------------\n",
      "2\n",
      "[[1497  401]\n",
      " [ 110   73]]\n",
      "0.2222222222222222\n",
      "--------------\n",
      "3\n",
      "[[1532  276]\n",
      " [ 222   51]]\n",
      "0.17\n",
      "--------------\n",
      "4\n",
      "[[1261  445]\n",
      " [ 185  190]]\n",
      "0.3762376237623763\n",
      "--------------\n",
      "5\n",
      "[[1553  364]\n",
      " [ 101   63]]\n",
      "0.21319796954314718\n",
      "--------------\n",
      "6\n",
      "[[1623  268]\n",
      " [ 137   53]]\n",
      "0.20743639921722112\n",
      "--------------\n",
      "7\n",
      "[[1197  374]\n",
      " [ 264  246]]\n",
      "0.4353982300884956\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for c in range(0, nCluster):\n",
    "    print(c)\n",
    "    print(metrics.confusion_matrix(ctestY[c], stackingVote[c].predict(ctestX[c])))\n",
    "    print(metrics.f1_score(ctestY[c], stackingVote[c].predict(ctestX[c])))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lrResultSMOTE.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(stackedModelsC, \"stackedModelsC.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoosting with SMOTE nominal values RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientResultSmoteN=joblib.load(\"gradientResultSMOTEN.pkl\")\n",
    "gradientModelsN = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ntrainX[c], ntrainY[c])\n",
    "    parameters = gradientResultSmoteN[c].best_params_\n",
    "    gbc = GradientBoostingClassifier(**parameters)\n",
    "    gbc.fit(x_res, y_res)\n",
    "    gradientModelsN.append(gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest with SMOTE nominal values RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestResultSmoteN=joblib.load(\"randomForestResultCSMOTEN.pkl\")\n",
    "rfModelsN = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ntrainX[c], ntrainY[c])\n",
    "    parameters = randomForestResultSmoteN[c].best_params_\n",
    "    rfc = RandomForestClassifier(**parameters)\n",
    "    rfc.fit(x_res, y_res)\n",
    "    rfModelsN.append(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lrResultSmoteN=joblib.load(\"lrResultCSMOTEN.pkl\")\n",
    "lrModelsN = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ntrainX[c], ntrainY[c])\n",
    "    parameters = lrResultSmoteN[c].best_params_\n",
    "    lrm = LogisticRegression(solver='saga', **parameters, max_iter=5000)\n",
    "    lrm.fit(x_res, y_res)\n",
    "    lrModelsN.append(lrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackingClassN = []\n",
    "stackingVote = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(nstackX[c], nstackY[c])\n",
    "    estimators = [\n",
    "        ('gbc', gradientModelsN[c]),\n",
    "        ('rf', rfModelsN[c]),\n",
    "        ('lr', lrModelsN[c]) ]\n",
    "    stackingModel = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=5000), cv='prefit', passthrough = True)\n",
    "    stackingClassN.append(stackingModel.fit(x_res, y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1488  425]\n",
      " [  88   80]]\n",
      "0.23774145616641898\n",
      "--------------\n",
      "1\n",
      "[[1392  471]\n",
      " [ 146   72]]\n",
      "0.18922470433639949\n",
      "--------------\n",
      "2\n",
      "[[1515  383]\n",
      " [  76  107]]\n",
      "0.3179791976225855\n",
      "--------------\n",
      "3\n",
      "[[1336  472]\n",
      " [ 160  113]]\n",
      "0.2634032634032634\n",
      "--------------\n",
      "4\n",
      "[[1263  443]\n",
      " [ 177  198]]\n",
      "0.3897637795275591\n",
      "--------------\n",
      "5\n",
      "[[1459  458]\n",
      " [  84   80]]\n",
      "0.22792022792022792\n",
      "--------------\n",
      "6\n",
      "[[1453  438]\n",
      " [ 105   85]]\n",
      "0.23842917251051896\n",
      "--------------\n",
      "7\n",
      "[[1144  427]\n",
      " [ 218  292]]\n",
      "0.47518307567127743\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for c in range(0, nCluster):\n",
    "    print(c)\n",
    "    print(metrics.confusion_matrix(ntestY[c], stackingClassN[c].predict(ntestX[c])))\n",
    "    print(metrics.f1_score(ntestY[c], stackingClassN[c].predict(ntestX[c])))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akos\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stackingVoteN = []\n",
    "for c in range(0, nCluster):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    x_res, y_res = sm.fit_resample(ntrainX[c], ntrainY[c])\n",
    "    voteModel = VotingClassifier(estimators = [\n",
    "        ('gbc', gradientModelsN[c]),\n",
    "        ('rf', rfModelsN[c]),\n",
    "        ('lr', lrModelsN[c]),\n",
    "        ], voting='hard')\n",
    "    stackingVoteN.append(voteModel.fit(x_res, y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[1424  489]\n",
      " [  85   83]]\n",
      "0.2243243243243243\n",
      "--------------\n",
      "1\n",
      "[[1469  394]\n",
      " [ 163   55]]\n",
      "0.16491754122938532\n",
      "--------------\n",
      "2\n",
      "[[1491  407]\n",
      " [  89   94]]\n",
      "0.27485380116959063\n",
      "--------------\n",
      "3\n",
      "[[1285  523]\n",
      " [ 168  105]]\n",
      "0.23307436182019978\n",
      "--------------\n",
      "4\n",
      "[[1229  477]\n",
      " [ 168  207]]\n",
      "0.3909348441926346\n",
      "--------------\n",
      "5\n",
      "[[1476  441]\n",
      " [  96   68]]\n",
      "0.20208023774145617\n",
      "--------------\n",
      "6\n",
      "[[1496  395]\n",
      " [ 116   74]]\n",
      "0.22458270106221548\n",
      "--------------\n",
      "7\n",
      "[[1120  451]\n",
      " [ 241  269]]\n",
      "0.4373983739837398\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for c in range(0, nCluster):\n",
    "    print(c)\n",
    "    print(metrics.confusion_matrix(ntestY[c], stackingVoteN[c].predict(ntestX[c])))\n",
    "    print(metrics.f1_score(ntestY[c], stackingVoteN[c].predict(ntestX[c])))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalModel = StackingClassifier(estimators=stackingClass, final_estimator=LogisticRegression(max_iter=5000), cv='prefit', passthrough = False)\n",
    "finalModel = finalModel.fit(cCluster, targetLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
